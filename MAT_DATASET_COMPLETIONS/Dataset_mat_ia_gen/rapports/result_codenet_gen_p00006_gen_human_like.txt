================================================================================
ANALYSE DES PRÉDICTIONS DE TOKENS
================================================================================

SCRIPT ORIGINAL:
--------------------------------------------------------------------------------
s = input()
print(s[::-1])
--------------------------------------------------------------------------------

INFORMATION SUR LA TOKENISATION:
--------------------------------------------------------------------------------
Tokenisation utilisée: tiktoken pour le modèle gpt-4o-mini
Il s'agit de la tokenisation officielle utilisée par les modèles OpenAI.
--------------------------------------------------------------------------------

TOKENS IDENTIFIÉS:
--------------------------------------------------------------------------------
Token 0: 's' (ID: 82)
Token 1: ' =' (ID: 314)
Token 2: ' input' (ID: 3422)
Token 3: '()\n' (ID: 1234)
Token 4: 'print' (ID: 1598)
Token 5: '(s' (ID: 1858)
Token 6: '[::-' (ID: 159890)
Token 7: '1' (ID: 16)
Token 8: '])' (ID: 4636)
--------------------------------------------------------------------------------

INFORMATIONS SUR LA MATRICE 2D DE LOG PROBABILITÉS:
--------------------------------------------------------------------------------
Dimensions de la matrice: 2 lignes x 5 tokens max
Nombre de tokens par ligne: [4, 5]
Convention de valeurs:
  - Log probabilité normale: Valeur réelle (typiquement entre -1 et -20)
  - Anomalie (token hors top-10): -50
  - Padding (positions vides): 100

Matrice 2D (format texte simplifiée):
[-10.00, -10.00, -10.00, -3.58, PAD, ]
[-2.29, -1.70, -2.28, -0.00, -0.26, ]

Note: La matrice a été sauvegardée au format numpy dans 'matrice_logprob.npy'
--------------------------------------------------------------------------------

ANALYSE TOKEN PAR TOKEN:
--------------------------------------------------------------------------------
Position 0: 's' (ID: 82) (token d'amorce)
--------------------------------------------------------------------------------

Position 1: ' =' (ID: 314) (token d'amorce)
--------------------------------------------------------------------------------

Position 2: ' input' (ID: 3422) (token d'amorce)
--------------------------------------------------------------------------------

Position 3: '()\n' (ID: 1234)
Prédiction principale: '("' (✗ INCORRECT)

Top 10 tokens les plus probables:
  1. '("' (logprob: -0.7077) [Prédiction principale]
  2. '().' (logprob: -1.4577)
  3. '('' (logprob: -3.3327)
  4. '.split' (logprob: -3.4577)
* 5. '()\n' (logprob: -3.5827)
  6. '_string' (logprob: -3.7077)
  7. 'String' (logprob: -4.7077)
  8. '_str' (logprob: -4.7077)
  9. '_list' (logprob: -4.8327)
  10. '(f' (logprob: -5.0827)

Le token attendu est à la position 5 dans les prédictions.

--------------------------------------------------------------------------------

Position 4: 'print' (ID: 1598)
Prédiction principale: 'print' (✓ CORRECT)

Top 10 tokens les plus probables:
* 1. 'print' (logprob: -2.2919) [Prédiction principale]
  2. 'n' (logprob: -2.6669)
  3. 's' (logprob: -2.6669)
  4. '#' (logprob: -2.7919)
  5. 'if' (logprob: -3.5419)
  6. 'result' (logprob: -3.5419)
  7. 'for' (logprob: -3.6669)
  8. 'count' (logprob: -3.9169)
  9. 'a' (logprob: -4.0419)
  10. 't' (logprob: -4.0419)

Le token attendu est à la position 1 dans les prédictions.

--------------------------------------------------------------------------------

Position 5: '(s' (ID: 1858)
Prédiction principale: '(s' (✓ CORRECT)

Top 10 tokens les plus probables:
* 1. '(s' (logprob: -1.7015) [Prédiction principale]
  2. '(count' (logprob: -2.5765)
  3. '("' (logprob: -2.7015)
  4. '(' (logprob: -2.9515)
  5. '(is' (logprob: -3.0765)
  6. '(f' (logprob: -3.3265)
  7. '(find' (logprob: -3.4515)
  8. '('' (logprob: -3.5765)
  9. '(len' (logprob: -3.7015)
  10. '(c' (logprob: -3.8265)

Le token attendu est à la position 1 dans les prédictions.

--------------------------------------------------------------------------------

Position 6: '[::-' (ID: 159890)
Prédiction principale: ')\n' (✗ INCORRECT)

Top 10 tokens les plus probables:
  1. ')\n' (logprob: -1.6592) [Prédiction principale]
  2. '.count' (logprob: -1.7842)
  3. '[' (logprob: -2.1592)
* 4. '[::-' (logprob: -2.2842)
  5. ')' (logprob: -2.5342)
  6. '.replace' (logprob: -2.9092)
  7. ')\n\n' (logprob: -3.0342)
  8. '[-' (logprob: -3.6592)
  9. '[:' (logprob: -3.9092)
  10. ',' (logprob: -4.0342)

Le token attendu est à la position 4 dans les prédictions.

--------------------------------------------------------------------------------

Position 7: '1' (ID: 16)
Prédiction principale: '1' (✓ CORRECT)

Top 10 tokens les plus probables:
* 1. '1' (logprob: -0.0036) [Prédiction principale]
  2. '2' (logprob: -5.7536)
  3. '3' (logprob: -8.1286)
  4. ' ' (logprob: -10.0036)
  5. '4' (logprob: -10.1286)
  6. '5' (logprob: -10.7536)
  7. '10' (logprob: -11.2536)
  8. '0' (logprob: -12.0036)
  9. '6' (logprob: -12.3786)
  10. '7' (logprob: -12.5036)

Le token attendu est à la position 1 dans les prédictions.

--------------------------------------------------------------------------------

Position 8: '])' (ID: 4636)
Prédiction principale: '])' (✓ CORRECT)

Top 10 tokens les plus probables:
* 1. '])' (logprob: -0.2636) [Prédiction principale]
  2. '])\n' (logprob: -1.6386)
  3. '])\n\n' (logprob: -4.0136)
  4. '].' (logprob: -4.3886)
  5. '],' (logprob: -5.7636)
  6. ']' (logprob: -6.0136)
  7. '])\n\n\n' (logprob: -7.3886)
  8. '][' (logprob: -8.2636)
  9. '][:' (logprob: -8.7636)
  10. ']).' (logprob: -10.0136)

Le token attendu est à la position 1 dans les prédictions.

--------------------------------------------------------------------------------


RÉSUMÉ:
--------------------------------------------------------------------------------
Total des tokens analysés: 6
Tokens correctement prédits (1ère position, stricte): 4
Tokens correctement prédits (1ère position, avec adaptation): 4
Tokens correctement prédits (top 10): 6
Précision stricte (1ère position): 66.67%
Précision adaptée (1ère position): 66.67%
Précision (top 10): 100.00%
================================================================================
