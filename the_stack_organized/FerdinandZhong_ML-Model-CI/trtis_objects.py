# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: api.proto, request_status.proto, model_config.proto, server_status.proto, grpc_service.proto
# plugin: python-betterproto
from dataclasses import dataclass
from typing import Dict, List

import betterproto


class InferRequestHeaderFlag(betterproto.Enum):
    FLAG_NONE = 0
    FLAG_SEQUENCE_START = 1
    FLAG_SEQUENCE_END = 2


class RequestStatusCode(betterproto.Enum):
    """
    @@@@.. cpp:enum:: RequestStatusCode@@@@   Status codes returned for
    inference server requests. The@@
    :cpp:enumerator:`RequestStatusCode::SUCCESS` status code indicates@@   not
    error, all other codes indicate an error.@@
    """

    # @@  .. cpp:enumerator:: RequestStatusCode::INVALID = 0@@@@     Invalid
    # status. Used internally but should not be returned as@@     part of a
    # :cpp:var:`RequestStatus`.@@
    INVALID = 0
    # @@  .. cpp:enumerator:: RequestStatusCode::SUCCESS = 1@@@@     Error code
    # indicating success.@@
    SUCCESS = 1
    # @@  .. cpp:enumerator:: RequestStatusCode::UNKNOWN = 2@@@@     Error code
    # indicating an unknown failure.@@
    UNKNOWN = 2
    # @@  .. cpp:enumerator:: RequestStatusCode::INTERNAL = 3@@@@     Error code
    # indicating an internal failure.@@
    INTERNAL = 3
    # @@  .. cpp:enumerator:: RequestStatusCode::NOT_FOUND = 4@@@@     Error code
    # indicating a resource or request was not found.@@
    NOT_FOUND = 4
    # @@  .. cpp:enumerator:: RequestStatusCode::INVALID_ARG = 5@@@@     Error
    # code indicating a failure caused by an unknown argument or@@     value.@@
    INVALID_ARG = 5
    # @@  .. cpp:enumerator:: RequestStatusCode::UNAVAILABLE = 6@@@@     Error
    # code indicating an unavailable resource.@@
    UNAVAILABLE = 6
    # @@  .. cpp:enumerator:: RequestStatusCode::UNSUPPORTED = 7@@@@     Error
    # code indicating an unsupported request or operation.@@
    UNSUPPORTED = 7
    # @@  .. cpp:enumerator:: RequestStatusCode::ALREADY_EXISTS = 8@@@@     Error
    # code indicating an already existing resource.@@
    ALREADY_EXISTS = 8


class DataType(betterproto.Enum):
    """
    @@@@.. cpp:enum:: DataType@@@@   Data types supported for input and output
    tensors.@@
    """

    # @@  .. cpp:enumerator:: DataType::INVALID = 0
    TYPE_INVALID = 0
    # @@  .. cpp:enumerator:: DataType::BOOL = 1
    TYPE_BOOL = 1
    # @@  .. cpp:enumerator:: DataType::UINT8 = 2
    TYPE_UINT8 = 2
    # @@  .. cpp:enumerator:: DataType::UINT16 = 3
    TYPE_UINT16 = 3
    # @@  .. cpp:enumerator:: DataType::UINT32 = 4
    TYPE_UINT32 = 4
    # @@  .. cpp:enumerator:: DataType::UINT64 = 5
    TYPE_UINT64 = 5
    # @@  .. cpp:enumerator:: DataType::INT8 = 6
    TYPE_INT8 = 6
    # @@  .. cpp:enumerator:: DataType::INT16 = 7
    TYPE_INT16 = 7
    # @@  .. cpp:enumerator:: DataType::INT32 = 8
    TYPE_INT32 = 8
    # @@  .. cpp:enumerator:: DataType::INT64 = 9
    TYPE_INT64 = 9
    # @@  .. cpp:enumerator:: DataType::FP16 = 10
    TYPE_FP16 = 10
    # @@  .. cpp:enumerator:: DataType::FP32 = 11
    TYPE_FP32 = 11
    # @@  .. cpp:enumerator:: DataType::FP64 = 12
    TYPE_FP64 = 12
    # @@  .. cpp:enumerator:: DataType::STRING = 13
    TYPE_STRING = 13


class ModelInstanceGroupKind(betterproto.Enum):
    KIND_AUTO = 0
    KIND_GPU = 1
    KIND_CPU = 2
    KIND_MODEL = 3


class ModelInputFormat(betterproto.Enum):
    FORMAT_NONE = 0
    FORMAT_NHWC = 1
    FORMAT_NCHW = 2


class ModelOptimizationPolicyModelPriority(betterproto.Enum):
    PRIORITY_DEFAULT = 0
    PRIORITY_MAX = 1
    PRIORITY_MIN = 2


class ModelSequenceBatchingControlKind(betterproto.Enum):
    CONTROL_SEQUENCE_START = 0
    CONTROL_SEQUENCE_READY = 1


class ModelReadyState(betterproto.Enum):
    """
    @@@@.. cpp:enum:: ModelReadyState@@@@   Readiness status for models.@@
    """

    # @@  .. cpp:enumerator:: ModelReadyState::MODEL_UNKNOWN = 0@@@@     The
    # model is in an unknown state. The model is not available for@@
    # inferencing.@@
    MODEL_UNKNOWN = 0
    # @@  .. cpp:enumerator:: ModelReadyState::MODEL_READY = 1@@@@     The model
    # is ready and available for inferencing.@@
    MODEL_READY = 1
    # @@  .. cpp:enumerator:: ModelReadyState::MODEL_UNAVAILABLE = 2@@@@     The
    # model is unavailable, indicating that the model failed to@@     load or has
    # been implicitly or explicitly unloaded. The model is@@     not available
    # for inferencing.@@
    MODEL_UNAVAILABLE = 2
    # @@  .. cpp:enumerator:: ModelReadyState::MODEL_LOADING = 3@@@@     The
    # model is being loaded by the inference server. The model is@@     not
    # available for inferencing.@@
    MODEL_LOADING = 3
    # @@  .. cpp:enumerator:: ModelReadyState::MODEL_UNLOADING = 4@@@@     The
    # model is being unloaded by the inference server. The model is@@     not
    # available for inferencing.@@
    MODEL_UNLOADING = 4


class ServerReadyState(betterproto.Enum):
    """
    @@@@.. cpp:enum:: ServerReadyState@@@@   Readiness status for the inference
    server.@@
    """

    # @@  .. cpp:enumerator:: ServerReadyState::SERVER_INVALID = 0@@@@     The
    # server is in an invalid state and will likely not@@     response correctly
    # to any requests.@@
    SERVER_INVALID = 0
    # @@  .. cpp:enumerator:: ServerReadyState::SERVER_INITIALIZING = 1@@@@
    # The server is initializing.@@
    SERVER_INITIALIZING = 1
    # @@  .. cpp:enumerator:: ServerReadyState::SERVER_READY = 2@@@@     The
    # server is ready and accepting requests.@@
    SERVER_READY = 2
    # @@  .. cpp:enumerator:: ServerReadyState::SERVER_EXITING = 3@@@@     The
    # server is exiting and will not respond to requests.@@
    SERVER_EXITING = 3
    # @@  .. cpp:enumerator:: ServerReadyState::SERVER_FAILED_TO_INITIALIZE =
    # 10@@@@     The server did not initialize correctly. Most requests will
    # fail.@@
    SERVER_FAILED_TO_INITIALIZE = 10


class ModelControlRequestType(betterproto.Enum):
    UNLOAD = 0
    LOAD = 1


@dataclass
class InferSharedMemory(betterproto.Message):
    """
    @@.. cpp:var:: message InferSharedMemory@@@@   The meta-data for the shared
    memory from which to read the input@@   data and/or write the output
    data.@@
    """

    # @@  .. cpp:var:: string name@@@@     The name given during registration of
    # a shared memory region that@@     holds the input data (or where the output
    # data should be written).@@
    name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: uint64 offset@@@@     The offset from the start of the
    # shared memory region.@@     start = offset, end = offset + size;@@
    offset: int = betterproto.uint64_field(2)
    # @@  .. cpp:var:: uint64 byte_size@@@@     Size of the memory block, in
    # bytes.@@
    byte_size: int = betterproto.uint64_field(3)


@dataclass
class InferRequestHeader(betterproto.Message):
    """
    @@@@.. cpp:var:: message InferRequestHeader@@@@   Meta-data for an
    inferencing request. The actual input data is@@   delivered separate from
    this header, in the HTTP body for an HTTP@@   request, or in the
    :cpp:var:`InferRequest` message for a gRPC request.@@
    """

    # @@  .. cpp:var:: uint64 id@@@@     The ID of the inference request. The
    # response of the request will@@     have the same ID in InferResponseHeader.
    # The request sender can use@@     the ID to correlate the response to
    # corresponding request if needed.@@
    id: int = betterproto.uint64_field(5)
    # @@  .. cpp:var:: uint32 flags@@@@     The flags associated with this
    # request. This field holds a bitwise-or@@     of all flag values.@@
    flags: int = betterproto.uint32_field(6)
    # @@  .. cpp:var:: uint64 correlation_id@@@@     The correlation ID of the
    # inference request. Default is 0, which@@     indictes that the request has
    # no correlation ID. The correlation ID@@     is used to indicate two or more
    # inference request are related to@@     each other. How this relationship is
    # handled by the inference@@     server is determined by the model's
    # scheduling policy.@@
    correlation_id: int = betterproto.uint64_field(4)
    # @@  .. cpp:var:: uint32 batch_size@@@@     The batch size of the inference
    # request. This must be >= 1. For@@     models that don't support batching,
    # batch_size must be 1.@@
    batch_size: int = betterproto.uint32_field(1)
    # @@  .. cpp:var:: Input input (repeated)@@@@     The input meta-data for the
    # inputs provided with the the inference@@     request.@@
    input: List["InferRequestHeaderInput"] = betterproto.message_field(2)
    # @@  .. cpp:var:: Output output (repeated)@@@@     The output meta-data for
    # the inputs provided with the the inference@@     request.@@
    output: List["InferRequestHeaderOutput"] = betterproto.message_field(3)


@dataclass
class InferRequestHeaderInput(betterproto.Message):
    """
    @@  .. cpp:var:: message Input@@@@     Meta-data for an input tensor
    provided as part of an inferencing@@     request.@@
    """

    # @@    .. cpp:var:: string name@@@@       The name of the input tensor.@@
    name: str = betterproto.string_field(1)
    # @@    .. cpp:var:: int64 dims (repeated)@@@@       The shape of the input
    # tensor, not including the batch dimension.@@       Optional if the model
    # configuration for this input explicitly@@       specifies all dimensions of
    # the shape. Required if the model@@       configuration for this input has
    # any wildcard dimensions (-1).@@
    dims: List[int] = betterproto.int64_field(2)
    # @@    .. cpp:var:: uint64 batch_byte_size@@@@       The size of the full
    # batch of the input tensor, in bytes.@@       Optional for tensors with
    # fixed-sized datatypes. Required@@       for tensors with a non-fixed-size
    # datatype (like STRING).@@
    batch_byte_size: int = betterproto.uint64_field(3)
    # @@    .. cpp:var:: InferSharedMemory shared_memory@@@@       It is the
    # location in shared memory that contains the tensor data@@       for this
    # input. Using shared memory is optional but if this@@       message is used,
    # all fields are required.@@
    shared_memory: "InferSharedMemory" = betterproto.message_field(4)


@dataclass
class InferRequestHeaderOutput(betterproto.Message):
    """
    @@  .. cpp:var:: message Output@@@@     Meta-data for a requested output
    tensor as part of an inferencing@@     request.@@
    """

    # @@    .. cpp:var:: string name@@@@       The name of the output tensor.@@
    name: str = betterproto.string_field(1)
    # @@    .. cpp:var:: Class cls@@@@       Optional. If defined return this
    # output as a classification@@       instead of raw data. The output tensor
    # will be interpreted as@@       probabilities and the classifications
    # associated with the@@       highest probabilities will be returned.@@
    cls: "InferRequestHeaderOutputClass" = betterproto.message_field(3)
    # @@    .. cpp:var:: InferSharedMemory shared_memory@@@@       It is the
    # location in shared memory that the result tensor data@@       for this
    # output will be written. Using shared memory is optional@@       but if this
    # message is used, all fields are required.@@
    shared_memory: "InferSharedMemory" = betterproto.message_field(4)


@dataclass
class InferRequestHeaderOutputClass(betterproto.Message):
    """
    @@    .. cpp:var:: message Class@@@@       Options for an output returned
    as a classification.@@
    """

    # @@      .. cpp:var:: uint32 count@@@@         Indicates how many
    # classification values should be returned@@         for the output. The
    # 'count' highest priority values are@@         returned.@@
    count: int = betterproto.uint32_field(1)


@dataclass
class InferResponseHeader(betterproto.Message):
    """
    @@@@.. cpp:var:: message InferResponseHeader@@@@   Meta-data for the
    response to an inferencing request. The actual output@@   data is delivered
    separate from this header, in the HTTP body for an HTTP@@   request, or in
    the :cpp:var:`InferResponse` message for a gRPC request.@@
    """

    # @@  .. cpp:var:: uint64 id@@@@     The ID of the inference response. The
    # response will have the same ID@@     as the ID of its originated request.
    # The request sender can use@@     the ID to correlate the response to
    # corresponding request if needed.@@
    id: int = betterproto.uint64_field(5)
    # @@  .. cpp:var:: string model_name@@@@     The name of the model that
    # produced the outputs.@@
    model_name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: int64 model_version@@@@     The version of the model that
    # produced the outputs.@@
    model_version: int = betterproto.int64_field(2)
    # @@  .. cpp:var:: uint32 batch_size@@@@     The batch size of the outputs.
    # This will always be equal to the@@     batch size of the inputs. For models
    # that don't support@@     batching the batch_size will be 1.@@
    batch_size: int = betterproto.uint32_field(3)
    # @@  .. cpp:var:: Output output (repeated)@@@@     The outputs, in the same
    # order as they were requested in@@     :cpp:var:`InferRequestHeader`.@@
    output: List["InferResponseHeaderOutput"] = betterproto.message_field(4)


@dataclass
class InferResponseHeaderOutput(betterproto.Message):
    """
    @@  .. cpp:var:: message Output@@@@     Meta-data for an output tensor
    requested as part of an inferencing@@     request.@@
    """

    # @@    .. cpp:var:: string name@@@@       The name of the output tensor.@@
    name: str = betterproto.string_field(1)
    # @@    .. cpp:var:: Raw raw@@@@       If specified deliver results for this
    # output as raw tensor data.@@       The actual output data is delivered in
    # the HTTP body for an HTTP@@       request, or in the
    # :cpp:var:`InferResponse` message for a gRPC@@       request. Only one of
    # 'raw' and 'batch_classes' may be specified.@@
    raw: "InferResponseHeaderOutputRaw" = betterproto.message_field(2)
    # @@    .. cpp:var:: Classes batch_classes (repeated)@@@@       If specified
    # deliver results for this output as classifications.@@       There is one
    # :cpp:var:`Classes` object for each batch entry in@@       the output. Only
    # one of 'raw' and 'batch_classes' may be@@       specified.@@
    batch_classes: List["InferResponseHeaderOutputClasses"] = betterproto.message_field(
        3
    )


@dataclass
class InferResponseHeaderOutputRaw(betterproto.Message):
    """
    @@    .. cpp:var:: message Raw@@@@       Meta-data for an output tensor
    being returned as raw data.@@
    """

    # @@      .. cpp:var:: int64 dims (repeated)@@@@         The shape of the
    # output tensor, not including the batch@@         dimension.@@
    dims: List[int] = betterproto.int64_field(1)
    # @@      .. cpp:var:: uint64 batch_byte_size@@@@         The full size of
    # the output tensor, in bytes. For a@@         batch output, this is the size
    # of the entire batch.@@
    batch_byte_size: int = betterproto.uint64_field(2)


@dataclass
class InferResponseHeaderOutputClass(betterproto.Message):
    """
    @@    .. cpp:var:: message Class@@@@       Information about each
    classification for this output.@@
    """

    # @@      .. cpp:var:: int32 idx@@@@         The classification index.@@
    idx: int = betterproto.int32_field(1)
    # @@      .. cpp:var:: float value@@@@         The classification value as a
    # float (typically a@@         probability).@@
    value: float = betterproto.float_field(2)
    # @@      .. cpp:var:: string label@@@@         The label for the class
    # (optional, only available if provided@@         by the model).@@
    label: str = betterproto.string_field(3)


@dataclass
class InferResponseHeaderOutputClasses(betterproto.Message):
    """
    @@    .. cpp:var:: message Classes@@@@       Meta-data for an output tensor
    being returned as classifications.@@
    """

    # @@      .. cpp:var:: Class cls (repeated)@@@@         The topk classes for
    # this output.@@
    cls: List["InferResponseHeaderOutputClass"] = betterproto.message_field(1)


@dataclass
class RequestStatus(betterproto.Message):
    """
    @@@@.. cpp:var:: message RequestStatus@@@@   Status returned for all
    inference server requests. The@@   RequestStatus provides a
    :cpp:enum:`RequestStatusCode`, an@@   optional status message, and server
    and request IDs.@@
    """

    # @@  .. cpp:var:: RequestStatusCode code@@@@     The status code.@@
    code: "RequestStatusCode" = betterproto.enum_field(1)
    # @@  .. cpp:var:: string msg@@@@     The optional status message.@@
    msg: str = betterproto.string_field(2)
    # @@  .. cpp:var:: string server_id@@@@     The identifying string for the
    # server that is returning@@     this status.@@
    server_id: str = betterproto.string_field(3)
    # @@  .. cpp:var:: string request_id@@@@     Unique identifier for the
    # request assigned by the inference@@     server. Value 0 (zero) indicates
    # the request ID is not known.@@
    request_id: int = betterproto.uint64_field(4)


@dataclass
class ModelInstanceGroup(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelInstanceGroup@@@@   A group of one or more
    instances of a model and resources made@@   available for those
    instances.@@
    """

    # @@  .. cpp:var:: string name@@@@     Optional name of this group of
    # instances. If not specified the@@     name will be formed as <model
    # name>_<group number>. The name of@@     individual instances will be
    # further formed by a unique instance@@     number and GPU index:@@
    name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: Kind kind@@@@     The kind of this instance group. Default
    # is KIND_AUTO. If@@     KIND_AUTO or KIND_GPU then both 'count' and 'gpu'
    # are valid and@@     may be specified. If KIND_CPU or KIND_MODEL only
    # 'count' is valid@@     and 'gpu' cannot be specified.@@
    kind: "ModelInstanceGroupKind" = betterproto.enum_field(4)
    # @@  .. cpp:var:: int32 count@@@@     For a group assigned to GPU, the
    # number of instances created for@@     each GPU listed in 'gpus'. For a
    # group assigned to CPU the number@@     of instances created. Default is 1.
    count: int = betterproto.int32_field(2)
    # @@  .. cpp:var:: int32 gpus (repeated)@@@@     GPU(s) where instances
    # should be available. For each GPU listed,@@     'count' instances of the
    # model will be available. Setting 'gpus'@@     to empty (or not specifying
    # at all) is eqivalent to listing all@@     available GPUs.@@
    gpus: List[int] = betterproto.int32_field(3)
    # @@  .. cpp:var:: string profile@@@@     For TensorRT models, using inputs
    # with dynamic shape, this@@     parameter specifies the optimization profile
    # to be set for this@@     instance group. This field should lie between 0
    # and@@     <TotalNumberOfOptimizationProfilesInPlanModel> - 1 and be
    # specified@@     only for TensorRT backend, otherwise an error will be
    # generated.@@
    profile: str = betterproto.string_field(5)


@dataclass
class ModelTensorReshape(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelTensorReshape@@@@   Reshape specification for
    input and output tensors.@@
    """

    # @@  .. cpp:var:: int64 shape (repeated)@@@@     The shape to use for
    # reshaping.@@
    shape: List[int] = betterproto.int64_field(1)


@dataclass
class ModelInput(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelInput@@@@   An input required by the model.@@
    """

    # @@  .. cpp:var:: string name@@@@     The name of the input.@@
    name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: DataType data_type@@@@     The data-type of the input.@@
    data_type: "DataType" = betterproto.enum_field(2)
    # @@  .. cpp:var:: Format format@@@@     The format of the input. Optional.@@
    format: "ModelInputFormat" = betterproto.enum_field(3)
    # @@  .. cpp:var:: int64 dims (repeated)@@@@     The dimensions/shape of the
    # input tensor that must be provided@@     when invoking the inference API
    # for this model.@@
    dims: List[int] = betterproto.int64_field(4)
    # @@  .. cpp:var:: ModelTensorReshape reshape@@@@     The shape expected for
    # this input by the backend. The input will@@     be reshaped to this before
    # being presented to the backend. The@@     reshape must have the same number
    # of elements as the input shape@@     specified by 'dims'. Optional.@@
    reshape: "ModelTensorReshape" = betterproto.message_field(5)


@dataclass
class ModelOutput(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelOutput@@@@   An output produced by the
    model.@@
    """

    # @@  .. cpp:var:: string name@@@@     The name of the output.@@
    name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: DataType data_type@@@@     The data-type of the output.@@
    data_type: "DataType" = betterproto.enum_field(2)
    # @@  .. cpp:var:: int64 dims (repeated)@@@@     The dimensions/shape of the
    # output tensor.@@
    dims: List[int] = betterproto.int64_field(3)
    # @@  .. cpp:var:: ModelTensorReshape reshape@@@@     The shape produced for
    # this output by the backend. The output will@@     be reshaped from this to
    # the shape specifed in 'dims' before being@@     returned in the inference
    # response. The reshape must have the same@@     number of elements as the
    # output shape specified by 'dims'. Optional.@@
    reshape: "ModelTensorReshape" = betterproto.message_field(5)
    # @@  .. cpp:var:: string label_filename@@@@     The label file associated
    # with this output. Should be specified only@@     for outputs that represent
    # classifications. Optional.@@
    label_filename: str = betterproto.string_field(4)


@dataclass
class ModelVersionPolicy(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelVersionPolicy@@@@   Policy indicating which
    versions of a model should be made@@   available by the inference server.@@
    """

    # @@    .. cpp:var:: Latest latest@@@@       Serve only latest version(s) of
    # the model.@@
    latest: "ModelVersionPolicyLatest" = betterproto.message_field(
        1, group="policy_choice"
    )
    # @@    .. cpp:var:: All all@@@@       Serve all versions of the model.@@
    all: "ModelVersionPolicyAll" = betterproto.message_field(2, group="policy_choice")
    # @@    .. cpp:var:: Specific specific@@@@       Serve only specific
    # version(s) of the model.@@
    specific: "ModelVersionPolicySpecific" = betterproto.message_field(
        3, group="policy_choice"
    )


@dataclass
class ModelVersionPolicyLatest(betterproto.Message):
    """
    @@  .. cpp:var:: message Latest@@@@     Serve only the latest version(s) of
    a model. This is@@     the default policy.@@
    """

    # @@    .. cpp:var:: uint32 num_versions@@@@       Serve only the
    # 'num_versions' highest-numbered versions. T@@       The default value of
    # 'num_versions' is 1, indicating that by@@       default only the single
    # highest-number version of a@@       model will be served.@@
    num_versions: int = betterproto.uint32_field(1)


@dataclass
class ModelVersionPolicyAll(betterproto.Message):
    """
    @@  .. cpp:var:: message All@@@@     Serve all versions of the model.@@
    """

    pass


@dataclass
class ModelVersionPolicySpecific(betterproto.Message):
    """
    @@  .. cpp:var:: message Specific@@@@     Serve only specific versions of
    the model.@@
    """

    # @@    .. cpp:var:: int64 versions (repeated)@@@@       The specific
    # versions of the model that will be served.@@
    versions: List[int] = betterproto.int64_field(1)


@dataclass
class ModelOptimizationPolicy(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelOptimizationPolicy@@@@   Optimization
    settings for a model. These settings control if/how a@@   model is
    optimized and prioritized by the backend framework when@@   it is loaded.@@
    """

    # @@  .. cpp:var:: Graph graph@@@@     The graph optimization setting for the
    # model. Optional.@@
    graph: "ModelOptimizationPolicyGraph" = betterproto.message_field(1)
    # @@  .. cpp:var:: ModelPriority priority@@@@     The priority setting for
    # the model. Optional.@@
    priority: "ModelOptimizationPolicyModelPriority" = betterproto.enum_field(2)
    # @@  .. cpp:var:: Cuda cuda@@@@     CUDA-specific optimization settings.
    # Optional.@@
    cuda: "ModelOptimizationPolicyCuda" = betterproto.message_field(3)
    # @@  .. cpp:var:: ExecutionAccelerators execution_accelerators@@@@     The
    # accelerators used for the model. Optional.@@
    execution_accelerators: "ModelOptimizationPolicyExecutionAccelerators" = betterproto.message_field(
        4
    )


@dataclass
class ModelOptimizationPolicyGraph(betterproto.Message):
    """
    @@@@  .. cpp:var:: message Graph@@@@     Enable generic graph optimization
    of the model. If not specified@@     the framework's default level of
    optimization is used. Currently@@     only supported for TensorFlow
    graphdef and savedmodel models and@@     causes XLA to be enabled/disabled
    for the model.@@
    """

    # @@    .. cpp:var:: int32 level@@@@       The optimization level. Defaults
    # to 0 (zero) if not specified.@@@@         - -1: Disabled@@         -  0:
    # Framework default@@         -  1+: Enable optimization level (greater
    # values indicate@@            higher optimization levels)@@
    level: int = betterproto.int32_field(1)


@dataclass
class ModelOptimizationPolicyCuda(betterproto.Message):
    """
    @@@@  .. cpp:var:: message Cuda@@@@     CUDA-specific optimization
    settings.@@
    """

    # @@    .. cpp:var:: bool graphs@@@@       Use CUDA graphs API to capture
    # model operations and execute@@       them more efficiently. Currently only
    # recognized by TensorRT@@       backend.@@
    graphs: bool = betterproto.bool_field(1)


@dataclass
class ModelOptimizationPolicyExecutionAccelerators(betterproto.Message):
    """
    @@@@  .. cpp:var:: message ExecutionAccelerators@@@@     Specify the
    preferred execution accelerators to be used to execute@@     the model.
    Currently only recognized by ONNX Runtime backend.@@@@     For ONNX Runtime
    backend, multiple execution accelerators may be set@@     for both GPU and
    CPU, in such case, the priority will be in the@@     following order:@@
    <gpu_execution_accelerator> (if instance is on GPU)@@         CUDA
    Execution Provider     (if instance is on GPU)@@
    <cpu_execution_accelerator>@@         Default CPU Execution Provider@@
    """

    # @@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)@@@@
    # The preferred execution provider to be used if the model instance@@
    # is deployed on GPU. The order priority implies priority, i.e. the@@
    # provider at the front has highest priority.@@@@       For ONNX Runtime
    # backend, possible value is "tensorrt" as name,@@       and no parameters
    # are required.@@@@       For TensorFlow backend, possible value is
    # "tensorrt" as name,@@       with the following parameters:@@
    # "precision_mode" The precission used for optimization.@@         The value
    # can be one of "FP32", "FP16" and "INT8".@@         Default value is
    # "FP32".@@@@         "minimum_segment_size" The smallest model subgraph that
    # will@@         be considered for optimization by TensorRT. Default value is
    # 3.@@@@         "max_workspace_size_bytes" The maximum GPU memory the
    # model@@         can use temporarily during execution. Default value is
    # 1GB.@@
    gpu_execution_accelerator: List[
        "ModelOptimizationPolicyExecutionAcceleratorsAccelerator"
    ] = betterproto.message_field(1)
    # @@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)@@@@
    # The preferred execution provider to be used if the model instance@@
    # is deployed on CPU, or if the model instance is deployed on GPU@@       but
    # both 'gpu_execution_accelerator' and CUDA Execution Provider@@       don't
    # support the operators. The order priority implies priority,@@       i.e.
    # the provider at the front has highest priority.@@@@       For ONNX Runtime
    # backend, possible value is "openvino" as name,@@       and no parameters
    # are required.@@
    cpu_execution_accelerator: List[
        "ModelOptimizationPolicyExecutionAcceleratorsAccelerator"
    ] = betterproto.message_field(2)


@dataclass
class ModelOptimizationPolicyExecutionAcceleratorsAccelerator(
    betterproto.Message
):
    """
    @@@@  .. cpp:var:: message Accelerator@@@@     Specify the accelerator to
    be used to execute the model.@@     Accelerator with the same name may
    accept different parameters@@     depending on the backends.@@
    """

    # @@    .. cpp:var:: string name@@@@       The name of the execution
    # accelerator.@@
    name: str = betterproto.string_field(1)
    # @@    .. cpp:var:: map<string, string> parameters@@@@       Additional
    # paremeters used to configure the accelerator.@@
    parameters: Dict[str, str] = betterproto.map_field(
        2, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


@dataclass
class ModelDynamicBatching(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelDynamicBatching@@@@   Dynamic batching
    configuration. These settings control how dynamic@@   batching operates for
    the model.@@
    """

    # @@  .. cpp:var:: int32 preferred_batch_size (repeated)@@@@     Preferred
    # batch sizes for dynamic batching. If a batch of one of@@     these sizes
    # can be formed it will be executed immediately.  If@@     not specified a
    # preferred batch size will be chosen automatically@@     based on model and
    # GPU characteristics.@@
    preferred_batch_size: List[int] = betterproto.int32_field(1)
    # @@  .. cpp:var:: uint64 max_queue_delay_microseconds@@@@     The maximum
    # time, in microseconds, a request will be delayed in@@     the scheduling
    # queue to wait for additional requests for@@     batching. Default is 0.@@
    max_queue_delay_microseconds: int = betterproto.uint64_field(2)


@dataclass
class ModelSequenceBatching(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelSequenceBatching@@@@   Sequence batching
    configuration. These settings control how sequence@@   batching operates
    for the model.@@
    """

    # @@  .. cpp:var:: uint64 max_sequence_idle_microseconds@@@@     The maximum
    # time, in microseconds, that a sequence is allowed to@@     be idle before
    # it is aborted. The inference server considers a@@     sequence idle when it
    # does not have any inference request queued@@     for the sequence. If this
    # limit is exceeded, the inference server@@     will free the batch slot
    # allocated by the sequence and make it@@     available for another sequence.
    # If not specified (or specified as@@     zero) a default value of 1000000 (1
    # second) is used.@@
    max_sequence_idle_microseconds: int = betterproto.uint64_field(1)
    # @@  .. cpp:var:: ControlInput control_input (repeated)@@@@     The model
    # input(s) that the server should use to communicate@@     sequence start,
    # stop, ready and similar control values to the@@     model.@@
    control_input: List[
        "ModelSequenceBatchingControlInput"
    ] = betterproto.message_field(2)


@dataclass
class ModelSequenceBatchingControl(betterproto.Message):
    """
    @@  .. cpp:var:: message Control@@@@     A control is a binary signal to a
    backend.@@
    """

    # @@    .. cpp:var:: Kind kind@@@@       The kind of this control.@@
    kind: "ModelSequenceBatchingControlKind" = betterproto.enum_field(1)
    # @@    .. cpp:var:: int32 int32_false_true (repeated)@@@@       The
    # control's true and false setting is indicated by setting@@       a value in
    # an int32 tensor. The tensor must be a@@       1-dimensional tensor with
    # size equal to the batch size of@@       the request. 'int32_false_true'
    # must have two entries: the@@       first the false value and the second the
    # true value.@@
    int32_false_true: List[int] = betterproto.int32_field(2)
    # @@    .. cpp:var:: float fp32_false_true (repeated)@@@@       The control's
    # true and false setting is indicated by setting@@       a value in a fp32
    # tensor. The tensor must be a@@       1-dimensional tensor with size equal
    # to the batch size of@@       the request. 'fp32_false_true' must have two
    # entries: the@@       first the false value and the second the true value.@@
    fp32_false_true: List[float] = betterproto.float_field(3)


@dataclass
class ModelSequenceBatchingControlInput(betterproto.Message):
    """
    @@  .. cpp:var:: message ControlInput@@@@     The sequence control values
    to communicate by a model input.@@
    """

    # @@    .. cpp:var:: string name@@@@       The name of the model input.@@
    name: str = betterproto.string_field(1)
    # @@    .. cpp:var:: Control control (repeated)@@@@       The control
    # value(s) that should be communicated to the@@       model using this model
    # input.@@
    control: List["ModelSequenceBatchingControl"] = betterproto.message_field(2)


@dataclass
class ModelEnsembling(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelEnsembling@@@@   Model ensembling
    configuration. These settings specify the models that@@   compose the
    ensemble and how data flows between the models.@@
    """

    # @@  .. cpp:var:: Step step (repeated)@@@@     The models and the input /
    # output mappings used within the ensemble.@@
    step: List["ModelEnsemblingStep"] = betterproto.message_field(1)


@dataclass
class ModelEnsemblingStep(betterproto.Message):
    """
    @@  .. cpp:var:: message Step@@@@     Each step specifies a model included
    in the ensemble,@@     maps ensemble tensor names to the model input
    tensors,@@     and maps model output tensors to ensemble tensor names@@
    """

    # @@  .. cpp:var:: string model_name@@@@     The name of the model to execute
    # for this step of the ensemble.@@
    model_name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: int64 model_version@@@@     The version of the model to
    # use for inference. If -1@@     the latest/most-recent version of the model
    # is used.@@
    model_version: int = betterproto.int64_field(2)
    # @@  .. cpp:var:: map<string,string> input_map@@@@     Map from name of an
    # input tensor on this step's model to ensemble@@     tensor name. The
    # ensemble tensor must have the same data type and@@     shape as the model
    # input. Each model input must be assigned to@@     one ensemble tensor, but
    # the same ensemble tensor can be assigned@@     to multiple model inputs.@@
    input_map: Dict[str, str] = betterproto.map_field(
        3, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    # @@  .. cpp:var:: map<string,string> output_map@@@@     Map from name of an
    # output tensor on this step's model to ensemble@@     tensor name. The data
    # type and shape of the ensemble tensor will@@     be inferred from the model
    # output. It is optional to assign all@@     model outputs to ensemble
    # tensors. One ensemble tensor name@@     can appear in an output map only
    # once.@@
    output_map: Dict[str, str] = betterproto.map_field(
        4, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


@dataclass
class ModelParameter(betterproto.Message):
    """@@@@.. cpp:var:: message ModelParameter@@@@   A model parameter.@@"""

    # @@  .. cpp:var:: string string_value@@@@     The string value of the
    # parameter.@@
    string_value: str = betterproto.string_field(1)


@dataclass
class ModelConfig(betterproto.Message):
    """@@@@.. cpp:var:: message ModelConfig@@@@   A model configuration.@@"""

    # @@  .. cpp:var:: string name@@@@     The name of the model.@@
    name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: string platform@@@@     The framework for the model.
    # Possible values are@@     "tensorrt_plan", "tensorflow_graphdef",@@
    # "tensorflow_savedmodel", "caffe2_netdef",@@     "onnxruntime_onnx",
    # "pytorch_libtorch" and "custom".@@
    platform: str = betterproto.string_field(2)
    # @@  .. cpp:var:: ModelVersionPolicy version_policy@@@@     Policy
    # indicating which version(s) of the model will be served.@@
    version_policy: "ModelVersionPolicy" = betterproto.message_field(3)
    # @@  .. cpp:var:: int32 max_batch_size@@@@     Maximum batch size allowed
    # for inference. This can only decrease@@     what is allowed by the model
    # itself. A max_batch_size value of 0@@     indicates that batching is not
    # allowed for the model and the@@     dimension/shape of the input and output
    # tensors must exactly@@     match what is specified in the input and output
    # configuration. A@@     max_batch_size value > 0 indicates that batching is
    # allowed and@@     so the model expects the input tensors to have an
    # additional@@     initial dimension for the batching that is not specified
    # in the@@     input (for example, if the model supports batched inputs of@@
    # 2-dimensional tensors then the model configuration will specify@@     the
    # input shape as [ X, Y ] but the model will expect the actual@@     input
    # tensors to have shape [ N, X, Y ]). For max_batch_size > 0@@     returned
    # outputs will also have an additional initial dimension@@     for the
    # batch.@@
    max_batch_size: int = betterproto.int32_field(4)
    # @@  .. cpp:var:: ModelInput input (repeated)@@@@     The inputs request by
    # the model.@@
    input: List["ModelInput"] = betterproto.message_field(5)
    # @@  .. cpp:var:: ModelOutput output (repeated)@@@@     The outputs produced
    # by the model.@@
    output: List["ModelOutput"] = betterproto.message_field(6)
    # @@  .. cpp:var:: ModelOptimizationPolicy optimization@@@@     Optimization
    # configuration for the model. If not specified@@     then default
    # optimization policy is used.@@
    optimization: "ModelOptimizationPolicy" = betterproto.message_field(12)
    # @@    .. cpp:var:: ModelDynamicBatching dynamic_batching@@@@       If
    # specified, enables the dynamic-batching scheduling@@       policy. With
    # dynamic-batching the scheduler may group@@       together independent
    # requests into a single batch to@@       improve inference throughput.@@
    dynamic_batching: "ModelDynamicBatching" = betterproto.message_field(
        11, group="scheduling_choice"
    )
    # @@    .. cpp:var:: ModelSequenceBatching sequence_batching@@@@       If
    # specified, enables the sequence-batching scheduling@@       policy. With
    # sequence-batching, inference requests@@       with the same correlation ID
    # are routed to the same@@       model instance. Multiple sequences of
    # inference requests@@       may be batched together into a single batch to@@
    # improve inference throughput.@@
    sequence_batching: "ModelSequenceBatching" = betterproto.message_field(
        13, group="scheduling_choice"
    )
    # @@    .. cpp:var:: ModelEnsembling ensemble_scheduling@@@@       If
    # specified, enables the model-ensembling scheduling@@       policy. With
    # model-ensembling, inference requests@@       will be processed according to
    # the specification, such as an@@       execution sequence of models. The
    # input specified in this model@@       config will be the input for the
    # ensemble, and the output@@       specified will be the output of the
    # ensemble.@@
    ensemble_scheduling: "ModelEnsembling" = betterproto.message_field(
        15, group="scheduling_choice"
    )
    # @@  .. cpp:var:: ModelInstanceGroup instance_group (repeated)@@@@
    # Instances of this model. If not specified, one instance@@     of the model
    # will be instantiated on each available GPU.@@
    instance_group: List["ModelInstanceGroup"] = betterproto.message_field(7)
    # @@  .. cpp:var:: string default_model_filename@@@@     Optional filename of
    # the model file to use if a@@     compute-capability specific model is not
    # specified in@@     :cpp:var:`cc_model_filenames`. If not specified the
    # default name@@     is 'model.graphdef', 'model.savedmodel', 'model.plan'
    # or@@     'model.netdef' depending on the model type.@@
    default_model_filename: str = betterproto.string_field(8)
    # @@  .. cpp:var:: map<string,string> cc_model_filenames@@@@     Optional map
    # from CUDA compute capability to the filename of@@     the model that
    # supports that compute capability. The filename@@     refers to a file
    # within the model version directory.@@
    cc_model_filenames: Dict[str, str] = betterproto.map_field(
        9, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    # @@  .. cpp:var:: map<string,string> metric_tags@@@@     Optional metric
    # tags. User-specific key-value pairs for metrics@@     reported for this
    # model. These tags are applied to the metrics@@     reported on the HTTP
    # metrics port.@@
    metric_tags: Dict[str, str] = betterproto.map_field(
        10, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    # @@  .. cpp:var:: map<string,ModelParameter> parameters@@@@     Optional
    # model parameters. User-specified parameter values that@@     are made
    # available to custom backends.@@
    parameters: Dict[str, "ModelParameter"] = betterproto.map_field(
        14, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )


@dataclass
class StatDuration(betterproto.Message):
    """
    @@@@.. cpp:var:: message StatDuration@@@@   Statistic collecting a duration
    metric.@@
    """

    # @@  .. cpp:var:: uint64 count@@@@     Cumulative number of times this
    # metric occurred.@@
    count: int = betterproto.uint64_field(1)
    # @@  .. cpp:var:: uint64 total_time_ns@@@@     Total collected duration of
    # this metric in nanoseconds.@@
    total_time_ns: int = betterproto.uint64_field(2)


@dataclass
class StatusRequestStats(betterproto.Message):
    """
    @@@@.. cpp:var:: message StatusRequestStats@@@@   Statistics collected for
    Status requests.@@
    """

    # @@  .. cpp:var:: StatDuration success@@@@     Total time required to handle
    # successful Status requests, not@@     including HTTP or gRPC endpoint
    # termination time.@@
    success: "StatDuration" = betterproto.message_field(1)


@dataclass
class HealthRequestStats(betterproto.Message):
    """
    @@@@.. cpp:var:: message HealthRequestStats@@@@   Statistics collected for
    Health requests.@@
    """

    # @@  .. cpp:var:: StatDuration success@@@@     Total time required to handle
    # successful Health requests, not@@     including HTTP or gRPC endpoint
    # termination time.@@
    success: "StatDuration" = betterproto.message_field(1)


@dataclass
class ModelControlRequestStats(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelControlRequestStats@@@@   Statistics
    collected for ModelControl requests.@@
    """

    # @@  .. cpp:var:: StatDuration success@@@@     Total time required to handle
    # successful ModelControl requests, not@@     including HTTP or gRPC endpoint
    # termination time.@@
    success: "StatDuration" = betterproto.message_field(1)


@dataclass
class SharedMemoryControlRequestStats(betterproto.Message):
    """
    @@@@.. cpp:var:: message SharedMemoryControlRequestStats@@@@   Statistics
    collected for SharedMemoryControl requests.@@
    """

    # @@  .. cpp:var:: StatDuration success@@@@     Total time required to handle
    # successful SharedMemoryControl@@     requests, not including HTTP or gRPC
    # endpoint termination time.@@
    success: "StatDuration" = betterproto.message_field(1)


@dataclass
class InferRequestStats(betterproto.Message):
    """
    @@@@.. cpp:var:: message InferRequestStats@@@@   Statistics collected for
    Infer requests.@@
    """

    # @@  .. cpp:var:: StatDuration success@@@@     Total time required to handle
    # successful Infer requests, not@@     including HTTP or GRPC endpoint
    # handling time.@@
    success: "StatDuration" = betterproto.message_field(1)
    # @@  .. cpp:var:: StatDuration failed@@@@     Total time required to handle
    # failed Infer requests, not@@     including HTTP or GRPC endpoint handling
    # time.@@
    failed: "StatDuration" = betterproto.message_field(2)
    # @@  .. cpp:var:: StatDuration compute@@@@     Time required to run
    # inferencing for an inference request;@@     including time copying input
    # tensors to GPU memory, time@@     executing the model, and time copying
    # output tensors from GPU@@     memory.@@
    compute: "StatDuration" = betterproto.message_field(3)
    # @@  .. cpp:var:: StatDuration queue@@@@     Time an inference request waits
    # in scheduling queue for an@@     available model instance.@@
    queue: "StatDuration" = betterproto.message_field(4)


@dataclass
class ModelVersionStatus(betterproto.Message):
    """
    @@@@.. cpp:var:: message ModelVersionStatus@@@@   Status for a version of a
    model.@@
    """

    # @@  .. cpp:var:: ModelReadyState ready_statue@@@@     Current readiness
    # state for the model.@@
    ready_state: "ModelReadyState" = betterproto.enum_field(1)
    # @@  .. cpp:var:: map<uint32, InferRequestStats> infer_stats@@@@
    # Inference statistics for the model, as a map from batch size@@     to the
    # statistics. A batch size will not occur in the map@@     unless there has
    # been at least one inference request of@@     that batch size.@@
    infer_stats: Dict[int, "InferRequestStats"] = betterproto.map_field(
        2, betterproto.TYPE_UINT32, betterproto.TYPE_MESSAGE
    )
    # @@  .. cpp:var:: uint64 model_execution_count@@@@     Cumulative number of
    # model executions performed for the@@     model. A single model execution
    # performs inferencing for@@     the entire request batch and can perform
    # inferencing for multiple@@     requests if dynamic batching is enabled.@@
    model_execution_count: int = betterproto.uint64_field(3)
    # @@  .. cpp:var:: uint64 model_inference_count@@@@     Cumulative number of
    # model inferences performed for the@@     model. Each inference in a batched
    # request is counted as@@     an individual inference.@@
    model_inference_count: int = betterproto.uint64_field(4)


@dataclass
class ModelStatus(betterproto.Message):
    """@@@@.. cpp:var:: message ModelStatus@@@@   Status for a model.@@"""

    # @@  .. cpp:var:: ModelConfig config@@@@     The configuration for the
    # model.@@
    config: "ModelConfig" = betterproto.message_field(1)
    # @@  .. cpp:var:: map<int64, ModelVersionStatus> version_status@@@@
    # Duration statistics for each version of the model, as a map@@     from
    # version to the status. A version will not occur in the map@@     unless
    # there has been at least one inference request of@@     that model version.
    # A version of -1 indicates the status is@@     for requests for which the
    # version could not be determined.@@
    version_status: Dict[int, "ModelVersionStatus"] = betterproto.map_field(
        2, betterproto.TYPE_INT64, betterproto.TYPE_MESSAGE
    )


@dataclass
class SharedMemoryRegion(betterproto.Message):
    """
    @@.. cpp:var:: message SharedMemoryRegion@@@@   The meta-data for the
    shared memory region registered in the inference@@   server.@@
    """

    # @@@@  .. cpp:var:: string name@@@@     The name for this shared memory
    # region.@@
    name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: string shared_memory_key@@@@     The name of the shared
    # memory region that holds the input data@@     (or where the output data
    # should be written).@@
    shared_memory_key: str = betterproto.string_field(2)
    # @@  .. cpp:var:: uint64 offset@@@@     The offset from the start of the
    # shared memory region.@@     start = offset, end = offset + size;@@
    offset: int = betterproto.uint64_field(3)
    # @@  .. cpp:var:: uint64 byte_size@@@@     Size of the memory block, in
    # bytes.@@
    byte_size: int = betterproto.uint64_field(4)


@dataclass
class ServerStatus(betterproto.Message):
    """
    @@@@.. cpp:var:: message ServerStatus@@@@   Status for the inference
    server.@@
    """

    # @@  .. cpp:var:: string id@@@@     The server's ID.@@
    id: str = betterproto.string_field(1)
    # @@  .. cpp:var:: string version@@@@     The server's version.@@
    version: str = betterproto.string_field(2)
    # @@  .. cpp:var:: ServerReadyState ready_state@@@@     Current readiness
    # state for the server.@@
    ready_state: "ServerReadyState" = betterproto.enum_field(7)
    # @@  .. cpp:var:: uint64 uptime_ns@@@@     Server uptime in nanoseconds.@@
    uptime_ns: int = betterproto.uint64_field(3)
    # @@  .. cpp:var:: map<string, ModelStatus> model_status@@@@     Status for
    # each model, as a map from model name to the@@     status.@@
    model_status: Dict[str, "ModelStatus"] = betterproto.map_field(
        4, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    # @@  .. cpp:var:: StatusRequestStats status_stats@@@@     Statistics for
    # Status requests.@@
    status_stats: "StatusRequestStats" = betterproto.message_field(5)
    # @@  .. cpp:var:: HealthRequestStats health_stats@@@@     Statistics for
    # Health requests.@@
    health_stats: "HealthRequestStats" = betterproto.message_field(8)
    # @@  .. cpp:var:: ModelControlRequestStats model_control_stats@@@@
    # Statistics for ModelControl requests.@@
    model_control_stats: "ModelControlRequestStats" = betterproto.message_field(9)
    # @@  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats@@@@
    # Statistics for SharedMemoryControl requests.@@
    shm_control_stats: "SharedMemoryControlRequestStats" = betterproto.message_field(10)


@dataclass
class SharedMemoryStatus(betterproto.Message):
    """
    @@@@.. cpp:var:: message SharedMemoryStatus@@@@   Shared memory status for
    the inference server.@@
    """

    # @@@@  .. cpp:var:: SharedMemoryRegion shared_memory_region@@@@     The list
    # of active/registered shared memory regions.@@
    shared_memory_region: List["SharedMemoryRegion"] = betterproto.message_field(2)


@dataclass
class StatusRequest(betterproto.Message):
    """
    @@ @@.. cpp:var:: message StatusRequest @@ @@   Request message for Status
    gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: string model_name @@ @@     The specific model status
    # to be returned. If empty return status @@     for all models. @@
    model_name: str = betterproto.string_field(1)


@dataclass
class StatusResponse(betterproto.Message):
    """
    @@ @@.. cpp:var:: message StatusResponse @@ @@   Response message for
    Status gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: RequestStatus request_status @@ @@     The status of
    # the request, indicating success or failure. @@
    request_status: "RequestStatus" = betterproto.message_field(1)
    # @@ @@  .. cpp:var:: ServerStatus server_status @@ @@     The server and
    # model status. @@
    server_status: "ServerStatus" = betterproto.message_field(2)


@dataclass
class HealthRequest(betterproto.Message):
    """
    @@ @@.. cpp:var:: message HealthRequest @@ @@   Request message for Health
    gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: string mode @@ @@     The requested health action:
    # 'live' requests the liveness @@     state of the inference server; 'ready'
    # requests the readiness state @@     of the inference server. @@
    mode: str = betterproto.string_field(1)


@dataclass
class HealthResponse(betterproto.Message):
    """
    @@ @@.. cpp:var:: message HealthResponse @@ @@   Response message for
    Health gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: RequestStatus request_status @@ @@     The status of
    # the request, indicating success or failure. @@
    request_status: "RequestStatus" = betterproto.message_field(1)
    # @@ @@  .. cpp:var:: bool health @@ @@     The result of the request. True
    # indicates the inference server is @@     live/ready, false indicates the
    # inference server is not live/ready. @@
    health: bool = betterproto.bool_field(2)


@dataclass
class ModelControlRequest(betterproto.Message):
    """
    @@ @@.. cpp:var:: message ModelControlRequest @@ @@   Request message for
    ModelControl gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: string model_name @@ @@     The target model name. @@
    model_name: str = betterproto.string_field(1)
    # @@ @@  .. cpp:var:: Type type @@ @@     The control type that is operated
    # on the specified model. @@
    type: "ModelControlRequestType" = betterproto.enum_field(2)


@dataclass
class ModelControlResponse(betterproto.Message):
    """
    @@ @@.. cpp:var:: message ModelControlResponse @@ @@   Response message for
    ModelControl gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: RequestStatus request_status @@ @@     The status of
    # the request, indicating success or failure. @@
    request_status: "RequestStatus" = betterproto.message_field(1)


@dataclass
class SharedMemoryControlRequest(betterproto.Message):
    """
    @@ @@.. cpp:var:: message SharedMemoryControlRequest @@ @@   Request
    message for managing registered shared memory regions in TRTIS. @@
    """

    # @@    .. cpp:var:: Register register @@ @@       To register the specified
    # shared memory region. @@
    register: "SharedMemoryControlRequestRegister" = betterproto.message_field(
        1, group="shared_memory_control"
    )
    # @@    .. cpp:var:: Unregister unregister @@ @@       To unregister the
    # specified shared memory region. @@
    unregister: "SharedMemoryControlRequestUnregister" = betterproto.message_field(
        2, group="shared_memory_control"
    )
    # @@    .. cpp:var:: UnregisterAll unregister_all @@ @@       To unregister
    # all active shared memory regions. @@
    unregister_all: "SharedMemoryControlRequestUnregisterAll" = betterproto.message_field(
        3, group="shared_memory_control"
    )
    # @@    .. cpp:var:: Status status @@ @@       Get the status of all active
    # shared memory regions. @@
    status: "SharedMemoryControlRequestStatus" = betterproto.message_field(
        4, group="shared_memory_control"
    )


@dataclass
class SharedMemoryControlRequestRegister(betterproto.Message):
    """
    @@  .. cpp:var:: message Register @@ @@     Register a shared memory
    region. @@
    """

    # @@ @@  .. cpp:var:: string name @@ @@     The name for this shared memory
    # region. @@
    name: str = betterproto.string_field(1)
    # @@ @@  .. cpp:var:: SystemSharedMemoryIdentifier system_shared_memory @@ @@
    # The identifier for this system shared memory region. @@
    system_shared_memory: "SharedMemoryControlRequestRegisterSystemSharedMemoryIdentifier" = betterproto.message_field(
        2, group="shared_memory_types"
    )
    # @@ @@  .. cpp:var:: CUDASharedMemoryIdentifier cuda_shared_memory @@ @@
    # The identifier for this CUDA shared memory region. @@
    cuda_shared_memory: "SharedMemoryControlRequestRegisterCUDASharedMemoryIdentifier" = betterproto.message_field(
        3, group="shared_memory_types"
    )
    # @@  .. cpp:var:: uint64 offset @@ @@     The offset from the start of the
    # shared memory region. @@     start = offset, end = offset + size; @@
    offset: int = betterproto.uint64_field(4)
    # @@  .. cpp:var:: uint64 byte_size @@ @@     Size of the memory block, in
    # bytes. @@
    byte_size: int = betterproto.uint64_field(5)


@dataclass
class SharedMemoryControlRequestRegisterSystemSharedMemoryIdentifier(
    betterproto.Message
):
    """
    @@ @@  .. cpp:var:: message SystemSharedMemoryIdentifier @@ @@     The
    identifier for this system shared memory region. @@
    """

    # @@  .. cpp:var:: string shared_memory_key @@ @@     The name of the shared
    # memory region that holds the input data @@     (or where the output data
    # should be written). @@
    shared_memory_key: str = betterproto.string_field(2)


@dataclass
class SharedMemoryControlRequestRegisterCUDASharedMemoryIdentifier(
    betterproto.Message
):
    """
    @@ @@  .. cpp:var:: message CUDASharedMemoryIdentifier @@ @@     The
    identifier for this system shared memory region. @@
    """

    # @@  .. cpp:var:: string shared_memory_key @@ @@     The name of the system
    # shared memory region that holds the @@     cudaIPC handle. @@
    shared_memory_key: str = betterproto.string_field(1)
    # @@  .. cpp:var:: uint64 offset @@ @@     The offset of the cudaIPC handle
    # from the start of the shared @@     memory region. @@     start = offset,
    # end = offset + size; @@
    offset: int = betterproto.uint64_field(2)
    # @@  .. cpp:var:: uint64 byte_size @@ @@     Size of the cudaIPC handle in
    # the shared memory block, in bytes. @@
    byte_size: int = betterproto.uint64_field(3)


@dataclass
class SharedMemoryControlRequestUnregister(betterproto.Message):
    """
    @@  .. cpp:var:: message Unregister @@ @@     Unregister a specified shared
    memory region. @@
    """

    # @@ @@  .. cpp:var:: string name @@ @@     The name for this shared memory
    # region to unregister. @@
    name: str = betterproto.string_field(1)


@dataclass
class SharedMemoryControlRequestUnregisterAll(betterproto.Message):
    """
    @@  .. cpp:var:: message UnregisterAll @@ @@     Unregister all shared
    memory regions. @@
    """

    pass


@dataclass
class SharedMemoryControlRequestStatus(betterproto.Message):
    """
    @@  .. cpp:var:: message GetStatus @@ @@     Get the status of all active
    shared memory regions. @@
    """

    pass


@dataclass
class SharedMemoryControlResponse(betterproto.Message):
    """
    @@ @@.. cpp:var:: message SharedMemoryControlResponse @@ @@   Response
    message for SharedMemoryControl gRPC endpoint. @@
    """

    # @@ @@  .. cpp:var:: RequestStatus request_status @@ @@     The status of
    # the request, indicating success or failure. @@
    request_status: "RequestStatus" = betterproto.message_field(1)
    # @@ @@  .. cpp:var:: Status shared_memory_status @@ @@     The status of all
    # active shared memory regions. @@
    shared_memory_status: "SharedMemoryControlResponseStatus" = betterproto.message_field(
        2, group="shared_memory_control"
    )


@dataclass
class SharedMemoryControlResponseStatus(betterproto.Message):
    """
    @@ @@.. cpp:var:: message Status @@ @@   Status of all active shared memory
    regions. @@
    """

    # @@ @@  .. cpp:var:: SharedMemoryRegion shared_memory_region @@ @@     The
    # list of active/registered shared memory regions. @@
    shared_memory_region: List["SharedMemoryRegion"] = betterproto.message_field(1)


@dataclass
class InferRequest(betterproto.Message):
    """
    @@ @@.. cpp:var:: message InferRequest @@ @@   Request message for Infer
    gRPC endpoint. @@
    """

    # @@  .. cpp:var:: string model_name @@ @@     The name of the model to use
    # for inferencing. @@
    model_name: str = betterproto.string_field(1)
    # @@  .. cpp:var:: int64 version @@ @@     The version of the model to use
    # for inference. If -1 @@     the latest/most-recent version of the model is
    # used. @@
    model_version: int = betterproto.int64_field(2)
    # @@  .. cpp:var:: InferRequestHeader meta_data @@ @@     Meta-data for the
    # request: input tensors, output @@     tensors, etc. @@
    meta_data: "InferRequestHeader" = betterproto.message_field(3)
    # @@  .. cpp:var:: bytes raw_input (repeated) @@ @@     The raw input tensor
    # data in the order specified in 'meta_data'. @@
    raw_input: List[bytes] = betterproto.bytes_field(4)


@dataclass
class InferResponse(betterproto.Message):
    """
    @@ @
    """

    # @@@@  .. cpp:var:: RequestStatus request_status@@@@     The status of the
    # request, indicating success or failure.@@
    request_status: "RequestStatus" = betterproto.message_field(1)
    # @@  .. cpp:var:: InferResponseHeader meta_data@@@@     The response meta-
    # data for the output tensors.@@
    meta_data: "InferResponseHeader" = betterproto.message_field(2)
    # @@  .. cpp:var:: bytes raw_output (repeated)@@@@     The raw output tensor
    # data in the order specified in 'meta_data'.@@
    raw_output: List[bytes] = betterproto.bytes_field(3)
